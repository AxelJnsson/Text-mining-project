{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset\n",
    "from sentence_transformers.losses import TripletLoss\n",
    "from sentence_transformers.readers import LabelSentenceReader, InputExample\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "# from sentence_transformers.examples.training.other.training_batch_hard_trec import triplets_from_labeled_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplets_from_labeled_dataset(input_examples):\n",
    "    # Copied from https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/other/training_batch_hard_trec.py\n",
    "    # Create triplets for a [(label, sentence), (label, sentence)...] dataset\n",
    "    # by using each example as an anchor and selecting randomly a\n",
    "    # positive instance with the same label and a negative instance with a different label\n",
    "    triplets = []\n",
    "    label2sentence = defaultdict(list)\n",
    "    for inp_example in input_examples:\n",
    "        label2sentence[inp_example.label].append(inp_example)\n",
    "\n",
    "    for inp_example in input_examples:\n",
    "        anchor = inp_example\n",
    "\n",
    "        if len(label2sentence[inp_example.label]) < 2: #We need at least 2 examples per label to create a triplet\n",
    "            continue\n",
    "\n",
    "        positive = None\n",
    "        while positive is None or positive.guid == anchor.guid:\n",
    "            positive = random.choice(label2sentence[inp_example.label])\n",
    "\n",
    "        negative = None\n",
    "        while negative is None or negative.label == anchor.label:\n",
    "            negative = random.choice(input_examples)\n",
    "\n",
    "        triplets.append(InputExample(texts=[anchor.texts[0], positive.texts[0], negative.texts[0]]))\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Load pre-trained model \n",
    "    sbert_model = SentenceTransformer('KBLab/sentence-bert-swedish-cased')\n",
    "\n",
    "    # Set up data for fine-tuning \n",
    "    sentence_reader = LabelSentenceReader(folder = './tsv_files', separator='#')\n",
    "    data_list = sentence_reader.get_examples(filename = 'scraping_results_clean_2.tsv')\n",
    "    triplets = triplets_from_labeled_dataset(input_examples = data_list)\n",
    "    finetune_data = SentencesDataset(examples = triplets, model = sbert_model)\n",
    "    finetune_dataloader = DataLoader(finetune_data, shuffle=True, batch_size=10)\n",
    "\n",
    "    # Initialize triplet loss\n",
    "    loss = TripletLoss(model=sbert_model)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    sbert_model.fit(train_objectives = [(finetune_dataloader, loss)], epochs = 4, output_path = 'fine_tuned_swedish_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the scraped questions and answers\n",
    "if not path.exists('fine_tuned_swedish_bert'):\n",
    "    print(\"The model does not exist. Wait for training\")\n",
    "    train()\n",
    "\n",
    "# Load the trained model\n",
    "model = SentenceTransformer('KBLab/sentence-bert-swedish-cased')\n",
    "ft_model = SentenceTransformer('fine_tuned_swedish_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions_and_answers_from_file(file_path):\n",
    "    '''\n",
    "    Loads questions and answers from file, also embedding questions\n",
    "    :param file_path: relative file_path to file (String)\n",
    "    :return question_embeddings: list of questions embedded by model\n",
    "    :return question_texts: list of string representation of questions\n",
    "    :return answer_mappings: list of string representation of answers \n",
    "    (question at index i of question_texts can be responded by answer at index i of answer_mappings)\n",
    "    '''\n",
    "    tsv_file = open(file_path)\n",
    "    read_tsv = csv.reader(tsv_file, delimiter=\"#\")\n",
    "\n",
    "    questions_and_answers = {}\n",
    "\n",
    "    for (key_str, text) in read_tsv:\n",
    "        key = int(key_str)\n",
    "        if key in questions_and_answers:\n",
    "            questions_and_answers[key].append(text)\n",
    "        else:\n",
    "            questions_and_answers[key] = [text]\n",
    "\n",
    "    question_embeddings = []\n",
    "    answer_mappings = []\n",
    "    \n",
    "\n",
    "    for i in tqdm(range(len(questions_and_answers))):\n",
    "        texts = questions_and_answers[i]\n",
    "        \n",
    "        answer = texts[-1]\n",
    "        for question in texts[:-1]:\n",
    "            question_embeddings.append(model.encode(question))\n",
    "            answer_mappings.append(answer)\n",
    "            \n",
    "\n",
    "    return question_embeddings, answer_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1785/1785 [00:59<00:00, 30.10it/s]\n"
     ]
    }
   ],
   "source": [
    "question_embeddings, answer_mappings = load_questions_and_answers_from_file(file_path=\"./tsv_files/scraping_results_clean_2.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(question):\n",
    "    '''\n",
    "    Returns the most appropriate answer to the question\n",
    "    :param question: Input question represented by string\n",
    "    '''\n",
    "    encoded_question_base = model.encode([question])\n",
    "    encoded_question_ft = ft_model.encode([question])\n",
    "\n",
    "    # We take 1 - cosine as spatial.distance.cdist calculates the distance, so the cosine similarity is 1 - distance\n",
    "    similarity_base = 1 - spatial.distance.cdist(np.array(encoded_question_base), np.array(question_embeddings), 'cosine')[0]\n",
    "    similarity_ft = 1 - spatial.distance.cdist(np.array(encoded_question_ft), np.array(question_embeddings), 'cosine')[0]\n",
    "    \n",
    "    results_base = zip(range(len(similarity_base)), similarity_base)\n",
    "    results_ft = zip(range(len(similarity_ft)), similarity_ft)\n",
    "\n",
    "    # Sort in reverse as we want to sort in descending order\n",
    "    results_base = sorted(results_base, key=lambda x: x[1], reverse=True)\n",
    "    results_ft = sorted(results_ft, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "\n",
    "    idx_base, similarity_base = results_base[0]\n",
    "    idx_ft, similarity_ft = results_ft[0]\n",
    "\n",
    "    # Check if the similarity is >0.5. If not, return string that informs the user that the question can't\n",
    "    # be answered, else return the answers\n",
    "    if similarity_base < 0.5:\n",
    "        answer_base = \"Jag kan tyvärr inte besvara din fråga. Om du tror att jag borde kunna besvara frågor inom detta ämne, testa att omformulera frågan.\"\n",
    "    else:\n",
    "        answer_base = answer_mappings[idx_base]\n",
    "\n",
    "    if similarity_ft < 0.5:\n",
    "        answer_ft = \"Jag kan tyvärr inte besvara din fråga. Om du tror att jag borde kunna besvara frågor inom detta ämne, testa att omformulera frågan.\"\n",
    "    else:\n",
    "        answer_ft = answer_mappings[idx_ft]\n",
    "\n",
    "    return [answer_base, answer_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: Myndigheten Trafikverket har inga APV-utbildningar som externa kan anmäla sig på. Ur konkurrenssynpunkt så får vi inte heller rekommendera en utbildare framför en annan. Däremot har vi tagit fram två stycken broschyrer som kan vara nyttiga att ta del av.\n",
      "\n",
      "FT model: Den kompetens inom \"Arbete på väg\" som Trafikverket kräver framgår i respektive kontrakt, med hänvisning till aktuellt kravdokument TDOK 2018:0371. Om du inte jobbar i ett kontrakt som Trafikverket upphandlat så omfattas du inte heller av Trafikverkets APV kompetenskrav. Då får du vända dig till din arbetsgivare/uppdragsgivare för att ta reda på vad som gäller i det just det arbete som du jobbar i och för de arbetsuppgifter som just du ska utföra  (till exempel med kommuner, elbolag, med flera).\n"
     ]
    }
   ],
   "source": [
    "# Test for model\n",
    "question = \"Vad erbjuder Trafikverket när det gäller APV-utbildningar för externa?\"\n",
    "\n",
    "answers = infer(question)\n",
    "answer_base = answers[0]\n",
    "answer_ft = answers[1]\n",
    "print(\"Base model: \" + answer_base + \"\\n\")\n",
    "print(\"FT model: \" + answer_ft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
